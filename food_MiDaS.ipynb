{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\USER\\Desktop\\food-detection\\120364.jpg: 384x640 3 bowls, 1 dining table, 20.9ms\n",
      "Speed: 1.0ms preprocess, 20.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Perform inference on an image\n",
    "img_path = \"./120364.jpg\"\n",
    "results = model(img_path)\n",
    "result = results[0]\n",
    "\n",
    "# Get the original image\n",
    "img = result.orig_img\n",
    "# Print and visualize the results\n",
    "# Loop over each result (for each image processed during inference)\n",
    "for box in result.boxes.xyxy:\n",
    "    x1, y1, x2, y2 = map(int, box[:4])\n",
    "    img = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "# Show the image with bounding boxes\n",
    "cv2.imshow(\"Detection Results\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squeezed input_img shape: torch.Size([1, 3, 384, 672])\n",
      "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]], device='cuda:0')\n",
      "food_region [[     1.2671      1.2717      1.2785 ...      5.7169      5.7217      5.7299]\n",
      " [     1.2886      1.2932      1.3002 ...      5.7445      5.7492      5.7576]\n",
      " [     1.3158      1.3204      1.3275 ...      5.7757      5.7802      5.7885]\n",
      " ...\n",
      " [     20.066      20.074      20.085 ...      19.588      19.582      19.576]\n",
      " [     20.105      20.112      20.124 ...      19.621      19.615      19.609]\n",
      " [     20.151      20.159       20.17 ...       19.66      19.655      19.648]]\n",
      "Bounding box: (244, 129, 820, 575)\n",
      "Estimated average depth: 18.594738006591797\n",
      "Estimated volume: 4776913.814941406\n",
      "\n",
      "food_region [[     7.3638       4.313     0.94502 ...           0   -0.093622    0.016628]\n",
      " [     7.7066      4.6725      1.0921 ...           0   -0.081904    0.014181]\n",
      " [     8.1768        5.18      1.3257 ...           0   -0.079392    0.013593]\n",
      " ...\n",
      " [     22.239      22.212      22.164 ...      21.034      21.034      21.039]\n",
      " [      22.25      22.231      22.188 ...      21.065      21.065      21.069]\n",
      " [     22.265      22.254      22.217 ...      21.094      21.094      21.097]]\n",
      "Bounding box: (211, 0, 850, 625)\n",
      "Estimated average depth: 15.156137466430664\n",
      "Estimated volume: 6052982.400655746\n",
      "\n",
      "food_region [[    0.73652     0.73616     0.73547 ...      2.6146      2.6148      2.6147]\n",
      " [    0.77098     0.77051     0.76973 ...      2.6577       2.657      2.6559]\n",
      " [     0.8008     0.80029     0.79949 ...      2.6953      2.6934      2.6911]\n",
      " ...\n",
      " [     14.572      14.575       14.58 ...      9.7951      9.7929      9.7905]\n",
      " [     14.579      14.582      14.586 ...      9.8486      9.8431      9.8389]\n",
      " [     14.588      14.591      14.595 ...      9.9012      9.8918      9.8865]]\n",
      "Bounding box: (508, 50, 758, 180)\n",
      "Estimated average depth: 9.113876342773438\n",
      "Estimated volume: 296200.9811401367\n",
      "\n",
      "food_region [[    0.73083     0.74203     0.70123 ...      1.8879      2.0289      2.2389]\n",
      " [    0.75104     0.76294      0.7224 ...       2.516      2.9403      3.3614]\n",
      " [    0.76581     0.77862     0.73848 ...      3.5586      4.1902      4.6945]\n",
      " ...\n",
      " [     8.3757      8.4006      8.3898 ...      20.348      20.375      20.396]\n",
      " [     8.3994       8.423      8.4117 ...      20.362      20.389       20.41]\n",
      " [     8.4273      8.4496      8.4376 ...      20.379      20.406      20.427]]\n",
      "Bounding box: (216, 105, 437, 265)\n",
      "Estimated average depth: 10.997048377990723\n",
      "Estimated volume: 388855.63064575195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get bounding boxes\n",
    "boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "# Load MiDaS model\n",
    "model_type = \"DPT_Large\"  # MiDaS v3 - Large\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Load the transforms to resize and normalize the image\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.dpt_transform if model_type == \"DPT_Large\" else midas_transforms.small_transform\n",
    "\n",
    "# Read and transform the input image\n",
    "img_cv2 = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
    "input_img = transform(img_rgb).unsqueeze(0).to(device)\n",
    "if input_img.shape[1] == 1:  # Assuming the second dimension is redundant\n",
    "    input_img = input_img.squeeze(1)  # Remove the second dimension\n",
    "    print(\"Squeezed input_img shape:\", input_img.shape)\n",
    "# Perform depth estimation\n",
    "with torch.no_grad():\n",
    "    print(input_img)\n",
    "    prediction = midas(input_img)\n",
    "\n",
    "    # Resize the prediction to original image size\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        prediction.unsqueeze(1),\n",
    "        size=img_rgb.shape[:2],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    ).squeeze()\n",
    "\n",
    "# Convert prediction to numpy array\n",
    "depth_map = prediction.cpu().numpy()\n",
    "\n",
    "# Iterate over detected bounding boxes\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = map(int, box[:4])\n",
    "\n",
    "    # Extract the region of interest (ROI) from the depth map\n",
    "    food_region = depth_map[y1:y2, x1:x2]\n",
    "    print(\"food_region\", food_region)\n",
    "\n",
    "    # Calculate the average depth of the food region\n",
    "    avg_depth = np.mean(food_region)\n",
    "\n",
    "    # Estimate the area of the food region (in pixels)\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # Estimate the volume (simplified approach)\n",
    "    volume_estimate = area * avg_depth\n",
    "\n",
    "    # Annotate the image with the volume estimate\n",
    "    label = f\"Volume: {volume_estimate:.2f} cm3\"\n",
    "    cv2.putText(img_cv2, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    print(f\"Bounding box: ({x1}, {y1}, {x2}, {y2})\")\n",
    "    print(f\"Estimated average depth: {avg_depth}\")\n",
    "    print(f\"Estimated volume: {volume_estimate}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Detected Objects and Volume', img_cv2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Export the YOLOv8 model to ONNX format\n",
    "# path = model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
